# KUMI Platform Improvements

This document outlines identified improvements for the KUMI knowledge base testing platform, organized by category with specific recommendations and code examples.

## üîß Code Quality & Maintainability

### Issues Identified
- **Excessive print statements**: 221+ `print()` calls across the codebase
- **Inconsistent logging patterns**: Mix of print() and proper logging
- **Missing comprehensive test suite**: Only individual test files found
- **Hardcoded values**: Magic numbers and strings scattered throughout code

### Recommendations

#### Replace Print Statements with Proper Logging
```python
# Current:
print(f"‚úÖ ChromaDBÂÆ¢Êà∑Á´ØÂàùÂßãÂåñÂÆåÊàê")

# Improved:
logger.info("ChromaDB client initialized successfully")

# Add structured logging with context
logger.info("Client initialized", extra={
    "client_type": "chroma",
    "host": self.host,
    "port": self.port
})
```

#### Add Unit and Integration Tests
```python
# tests/conftest.py
import pytest
from fastapi.testclient import TestClient

@pytest.fixture
def test_client():
    return TestClient(app)

# tests/test_knowledge_service.py
class TestKnowledgeService:
    def test_search_successful(self, knowledge_service, mock_request):
        result = await knowledge_service.search(mock_request)
        assert len(result.records) > 0
        assert result.records[0].score > 0.0
```

#### Extract Configuration Constants
```python
# config/constants.py
class Defaults:
    MAX_BATCH_SIZE = 20
    DEFAULT_TOP_K = 10
    MIN_CHUNK_SIZE = 100
    MAX_RETRIES = 3
    TIMEOUT_SECONDS = 30

# Usage in code
from config.constants import Defaults

batch_size = kwargs.get('batch_size', Defaults.MAX_BATCH_SIZE)
```

---

## üèóÔ∏è Architecture & Design

### Issues Identified
- **Tight coupling**: Services directly instantiate dependencies
- **Missing dependency injection**: Factory patterns are inconsistent
- **Singleton pattern misuse**: Complex singleton implementation in VectorDBFactory
- **Missing interfaces**: Some components lack proper abstractions

### Recommendations

#### Implement Dependency Injection
```python
# Current:
class KnowledgeService:
    def __init__(self):
        self.vector_client = VectorDBFactory.create_client()
        self.embedding_service = EmbeddingService()

# Improved:
class KnowledgeService:
    def __init__(self,
                 vector_client: VectorDBInterface,
                 embedding_service: EmbeddingServiceInterface):
        self.vector_client = vector_client
        self.embedding_service = embedding_service

# Use dependency injection container
# services/container.py
from dependency_injector import containers, providers

class Container(containers.DeclarativeContainer):
    config = providers.Configuration()

    vector_db_client = providers.Factory(
        VectorDBFactory.create_client,
        db_type=config.vector_db_type
    )

    knowledge_service = providers.Factory(
        KnowledgeService,
        vector_client=vector_db_client,
        embedding_service=embedding_service
    )
```

#### Simplify Singleton Pattern
```python
# Current: Complex singleton with double-checked locking
# Simplified: Use module-level singleton
# vector_db/factory.py
class VectorDBFactory:
    _instance = None
    _clients = {}

    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance

    def create_client(self, db_type: Optional[str] = None):
        # Implementation
        pass
```

#### Define Clear Interfaces
```python
# services/interfaces.py
from abc import ABC, abstractmethod
from typing import List, Dict, Any, Optional

class EmbeddingServiceInterface(ABC):
    @abstractmethod
    def encode_texts(self, texts: List[str], **kwargs) -> List[List[float]]:
        """Encode texts to embeddings"""
        pass

    @abstractmethod
    def get_client(self, provider: str = None, model: str = None):
        """Get embedding client"""
        pass

class VectorDBInterface(ABC):
    # Existing interface methods
    @abstractmethod
    def query_by_vector(self, collection_name: str, query_vector: List[float],
                        top_k: int = 10, **kwargs) -> List[Dict[str, Any]]:
        pass
```

---

## üîí Security

### Issues Identified
- **Weak authentication**: Simple token-based auth with hardcoded tokens
- **Token exposure in logs**: API keys partially logged
- **Missing rate limiting**: No protection against API abuse
- **Insufficient input validation**: Basic validation only

### Recommendations

#### Implement JWT-based Authentication
```python
# services/auth_service.py
import jwt
from datetime import datetime, timedelta
from passlib.context import CryptContext

pwd_context = CryptContext(schemes=["bcrypt"], deprecated="auto")

class AuthService:
    SECRET_KEY = os.getenv("SECRET_KEY")
    ALGORITHM = "HS256"

    def create_access_token(self, data: dict, expires_delta: Optional[timedelta] = None) -> str:
        to_encode = data.copy()
        expire = datetime.utcnow() + (expires_delta or timedelta(minutes=15))
        to_encode.update({"exp": expire})
        return jwt.encode(to_encode, self.SECRET_KEY, algorithm=self.ALGORITHM)

    def verify_token(self, token: str) -> Optional[str]:
        try:
            payload = jwt.decode(token, self.SECRET_KEY, algorithms=[self.ALGORITHM])
            return payload.get("sub")
        except JWTError:
            return None

    def hash_password(self, password: str) -> str:
        return pwd_context.hash(password)

    def verify_password(self, plain_password: str, hashed_password: str) -> bool:
        return pwd_context.verify(plain_password, hashed_password)
```

#### Add Rate Limiting
```python
# api/rate_limit.py
from slowapi import Limiter
from slowapi.util import get_remote_address
from slowapi.errors import RateLimitExceeded
from fastapi import Request

limiter = Limiter(key_func=get_remote_address)

# Usage in endpoints
@router.post("/api/v1/knowledge/retrieval")
@limiter.limit("10/minute")
async def retrieval(request: Request, retrieval_request: RetrievalRequest):
    # Implementation
    pass
```

#### Improve Input Validation
```python
# Add comprehensive validation to Pydantic models
from pydantic import BaseModel, Field, validator

class RetrievalRequest(BaseModel):
    knowledge_id: str = Field(..., min_length=3, max_length=100)
    query: str = Field(..., min_length=1, max_length=2000)
    retrieval_setting: RetrievalSetting
    metadata_condition: Optional[MetadataFilter] = None

    @validator('knowledge_id')
    def validate_knowledge_id(cls, v):
        if not re.match(r'^[a-zA-Z0-9_-]+$', v):
            raise ValueError('Invalid knowledge_id format')
        return v

    @validator('query')
    def sanitize_query(cls, v):
        # Remove potential injection attempts
        return re.sub(r'[<>"\'&]', '', v)
```

#### Secure Logging
```python
# Avoid logging sensitive data
logger.info("API key authenticated", extra={
    "token_prefix": token[:4] + "***" + token[-4:],  # Show only partial
    "user_agent": request.headers.get("user-agent")
})

# Never log full API keys, passwords, or tokens
```

---

## ‚ö° Performance

### Issues Identified
- **No connection pooling**: Each request creates new connections
- **Missing caching**: Repeated expensive operations
- **Synchronous operations**: Some blocking I/O operations
- **Large batch operations**: No chunking for large datasets

### Recommendations

#### Add Connection Pooling
```python
# services/database.py
from sqlalchemy import create_engine
from sqlalchemy.pool import QueuePool

class DatabaseService:
    def __init__(self, database_url: str):
        self.engine = create_engine(
            database_url,
            poolclass=QueuePool,
            pool_size=10,
            max_overflow=20,
            pool_pre_ping=True,
            pool_recycle=3600
        )
```

#### Implement Caching Layer
```python
# services/cache_service.py
from functools import lru_cache
import redis
import json

class CacheService:
    def __init__(self, redis_url: str = "redis://localhost:6379"):
        self.redis_client = redis.from_url(redis_url)
        self.default_ttl = 3600  # 1 hour

    def get(self, key: str) -> Optional[Any]:
        cached = self.redis_client.get(key)
        if cached:
            return json.loads(cached)
        return None

    def set(self, key: str, value: Any, ttl: int = None):
        ttl = ttl or self.default_ttl
        self.redis_client.setex(key, ttl, json.dumps(value))

    def cache_model_info(self, provider: str, model: str, info: dict):
        cache_key = f"embedding:{provider}:{model}"
        self.set(cache_key, info, ttl=7200)  # 2 hours
```

#### Add Async Operations Throughout
```python
# Convert blocking operations to async
import asyncio
from aiohttp import ClientSession

class AsyncEmbeddingClient:
    def __init__(self, base_url: str, api_key: str):
        self.base_url = base_url
        self.api_key = api_key

    async def encode_batch_async(self, texts: List[str], session: ClientSession) -> List[List[float]]:
        async with session.post(
            f"{self.base_url}/embeddings",
            headers={"Authorization": f"Bearer {self.api_key}"},
            json={"input": texts}
        ) as response:
            data = await response.json()
            return [item["embedding"] for item in data["data"]]

    async def encode_texts_async(self, texts: List[str], batch_size: int = 20) -> List[List[float]]:
        all_embeddings = []
        async with ClientSession() as session:
            for i in range(0, len(texts), batch_size):
                batch = texts[i:i + batch_size]
                embeddings = await self.encode_batch_async(batch, session)
                all_embeddings.extend(embeddings)
        return all_embeddings
```

#### Optimize Batch Operations
```python
# Add chunking for large datasets
async def process_large_dataset(self, data: List[Dict], chunk_size: int = 1000):
    """Process large datasets in chunks to avoid memory issues"""
    total_chunks = (len(data) + chunk_size - 1) // chunk_size

    for i in range(0, len(data), chunk_size):
        chunk = data[i:i + chunk_size]
        chunk_num = i // chunk_size + 1

        logger.info(f"Processing chunk {chunk_num}/{total_chunks}")
        await self._process_chunk(chunk)

        # Allow event loop to process other tasks
        await asyncio.sleep(0)
```

---

## üß™ Testing & Quality Assurance

### Issues Identified
- **No comprehensive test suite**: Missing unit/integration tests
- **No test coverage reporting**: Unknown test coverage
- **No CI/CD pipeline**: No automated testing
- **Missing test fixtures**: No test data management

### Recommendations

#### Implement Test Infrastructure
```python
# tests/conftest.py
import pytest
from fastapi.testclient import TestClient
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker

@pytest.fixture(scope="session")
def test_db():
    engine = create_engine("sqlite:///:memory:")
    Base.metadata.create_all(engine)
    Session = sessionmaker(bind=engine)
    yield Session()
    Base.metadata.drop_all(engine)

@pytest.fixture
def test_client():
    return TestClient(app)

@pytest.fixture
def mock_embedding_service():
    with patch('services.embedding_service.EmbeddingService') as mock:
        mock.return_value.encode_texts.return_value = [[0.1, 0.2, 0.3]]
        yield mock
```

#### Add Unit Tests
```python
# tests/test_knowledge_service.py
class TestKnowledgeService:
    def test_search_success(self, knowledge_service, mock_request):
        result = await knowledge_service.search(mock_request)
        assert len(result.records) > 0
        assert result.records[0].score > 0.0

    def test_search_empty_results(self, knowledge_service, mock_request_empty):
        result = await knowledge_service.search(mock_request_empty)
        assert len(result.records) == 0

    def test_search_with_filters(self, knowledge_service, mock_request_with_filters):
        result = await knowledge_service.search(mock_request_with_filters)
        for record in result.records:
            assert record.metadata["category"] == "test"
```

#### Add Integration Tests
```python
# tests/integration/test_full_workflow.py
class TestFullWorkflow:
    async def test_knowledge_retrieval_workflow(self, test_client):
        # Create collection
        create_response = test_client.post(
            "/api/v1/knowledge/collections",
            json={"name": "test_collection", "dimension": 768}
        )
        assert create_response.status_code == 200

        # Insert data
        insert_response = test_client.post(
            "/api/v1/knowledge/insert",
            json={"collection": "test_collection", "data": test_data}
        )
        assert insert_response.status_code == 200

        # Search
        search_response = test_client.post(
            "/api/v1/knowledge/retrieval",
            json={
                "knowledge_id": "test_collection",
                "query": "test query",
                "retrieval_setting": {"top_k": 5, "score_threshold": 0.7}
            }
        )
        assert search_response.status_code == 200
        assert len(search_response.json()["records"]) > 0
```

#### Setup CI/CD Pipeline
```yaml
# .github/workflows/ci.yml
name: CI

on: [push, pull_request]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.12'

      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          pip install pytest pytest-cov pytest-asyncio

      - name: Run tests with coverage
        run: |
          pytest tests/ --cov=api --cov=services --cov=vector_db --cov-report=xml

      - name: Upload coverage
        uses: codecov/codecov-action@v3
```

---

## üìù Configuration & Deployment

### Issues Identified
- **Configuration scattered**: Multiple config files and patterns
- **No environment validation**: Missing validation on startup
- **Docker support missing**: No containerization
- **Health checks minimal**: Basic health endpoints only

### Recommendations

#### Centralized Configuration with Validation
```python
# config/validation.py
from pydantic import BaseSettings, validator, Field

class Settings(BaseSettings):
    # Application
    APP_NAME: str = "KUMI"
    DEBUG: bool = False
    HOST: str = "0.0.0.0"
    PORT: int = 8000

    # Database
    DATABASE_URL: str = Field(..., env="DATABASE_URL")
    REDIS_URL: str = Field(..., env="REDIS_URL")

    # API Keys
    OPENAI_API_KEY: str = Field(..., env="OPENAI_API_KEY")

    @validator('DATABASE_URL')
    def validate_database_url(cls, v):
        if not v.startswith(('postgresql://', 'mysql://', 'sqlite://')):
            raise ValueError('Invalid database URL')
        return v

    @validator('OPENAI_API_KEY')
    def validate_api_key(cls, v):
        if not v or len(v) < 20:
            raise ValueError('Invalid API key')
        return v

    class Config:
        env_file = '.env'
        env_file_encoding = 'utf-8'

settings = Settings()
```

#### Add Docker Support
```dockerfile
# Dockerfile
FROM python:3.12-slim

WORKDIR /app

# Install system dependencies
RUN apt-get update && apt-get install -y \
    gcc \
    && rm -rf /var/lib/apt/lists/*

# Install Python dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy application code
COPY . .

# Create non-root user
RUN useradd -m appuser && chown -R appuser:appuser /app
USER appuser

# Expose port
EXPOSE 8000

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
    CMD curl -f http://localhost:8000/health || exit 1

# Run application
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]
```

#### Docker Compose Configuration
```yaml
# docker-compose.yml
version: '3.8'

services:
  kumi:
    build: .
    ports:
      - "8000:8000"
    environment:
      - DATABASE_URL=postgresql://user:pass@db:5432/kumi
      - REDIS_URL=redis://redis:6379
      - CHROMA_HOST=chroma
      - CHROMA_PORT=8081
    depends_on:
      db:
        condition: service_healthy
      redis:
        condition: service_started
      chroma:
        condition: service_started
    volumes:
      - ./output:/app/output
      - ./storage:/app/storage

  db:
    image: postgres:15
    environment:
      - POSTGRES_USER=user
      - POSTGRES_PASSWORD=pass
      - POSTGRES_DB=kumi
    ports:
      - "5432:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U user"]
      interval: 10s
      timeout: 5s
      retries: 5

  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"

  chroma:
    image: chromadb/chroma:latest
    ports:
      - "8081:8000"
    volumes:
      - chroma_data:/chroma/chroma

volumes:
  chroma_data:
```

#### Enhanced Health Checks
```python
# api/health.py
from fastapi import APIRouter
from services.cache_service import CacheService
from vector_db.factory import VectorDBFactory

router = APIRouter()

@router.get("/health")
async def health_check():
    checks = {
        "status": "healthy",
        "checks": {}
    }

    # Database check
    try:
        vector_client = VectorDBFactory.create_client()
        collections = vector_client.list_collections()
        checks["checks"]["vector_db"] = {
            "status": "healthy",
            "collections": len(collections)
        }
    except Exception as e:
        checks["checks"]["vector_db"] = {
            "status": "unhealthy",
            "error": str(e)
        }
        checks["status"] = "degraded"

    # Cache check
    try:
        cache = CacheService()
        cache.ping()
        checks["checks"]["cache"] = {"status": "healthy"}
    except Exception as e:
        checks["checks"]["cache"] = {
            "status": "unhealthy",
            "error": str(e)
        }

    # LLM check
    try:
        llm_client = LLMFactory.create_client()
        checks["checks"]["llm"] = {"status": "healthy"}
    except Exception as e:
        checks["checks"]["llm"] = {
            "status": "unhealthy",
            "error": str(e)
        }

    status_code = 200 if checks["status"] == "healthy" else 503
    return checks, status_code
```

---

## üîÑ Code Organization

### Issues Identified
- **Inconsistent module structure**: Mixed organization patterns
- **Missing __init__.py exports**: Unclear public interfaces
- **Large service classes**: Single Responsibility Principle violations
- **No clear domain boundaries**: Business logic mixed with infrastructure

### Recommendations

#### Improve Module Organization
```python
# services/__init__.py
"""
Services layer - Business logic and orchestration
"""
from .knowledge_service import KnowledgeService
from .embedding_service import EmbeddingService
from .similarity_service import SimilarityCalculator
from .cache_service import CacheService

__all__ = [
    'KnowledgeService',
    'EmbeddingService',
    'SimilarityCalculator',
    'CacheService'
]
```

#### Split Large Services
```python
# services/knowledge/__init__.py
from .search_service import SearchService
from .collection_service import CollectionService
from .metadata_service import MetadataService

__all__ = ['SearchService', 'CollectionService', 'MetadataService']

# services/knowledge/search_service.py
class SearchService:
    """Handles knowledge base search operations"""
    def __init__(self, vector_client, embedding_service):
        self.vector_client = vector_client
        self.embedding_service = embedding_service

    async def search(self, query: str, collection: str, **kwargs):
        # Search logic
        pass

# services/knowledge/collection_service.py
class CollectionService:
    """Handles collection CRUD operations"""
    def __init__(self, vector_client):
        self.vector_client = vector_client

    async def create_collection(self, name: str, **kwargs):
        # Collection creation logic
        pass
```

#### Define Clear Domain Boundaries
```python
# domain/knowledge/models.py
from dataclasses import dataclass
from typing import List, Optional
from datetime import datetime

@dataclass
class KnowledgeDocument:
    """Domain model for a knowledge document"""
    id: str
    content: str
    metadata: dict
    embedding: Optional[List[float]] = None
    created_at: Optional[datetime] = None
    updated_at: Optional[datetime] = None

@dataclass
class SearchResult:
    """Domain model for search results"""
    document: KnowledgeDocument
    score: float
    metadata: dict

# domain/knowledge/repositories.py
from abc import ABC, abstractmethod

class KnowledgeRepository(ABC):
    """Repository interface for knowledge documents"""

    @abstractmethod
    async def save(self, document: KnowledgeDocument) -> str:
        pass

    @abstractmethod
    async def find_by_id(self, doc_id: str) -> Optional[KnowledgeDocument]:
        pass

    @abstractmethod
    async def search_similar(self, embedding: List[float], top_k: int) -> List[SearchResult]:
        pass
```

---

## üöÄ Modernization Opportunities

### Recommendations

#### Use Modern Python Features
```python
# Use dataclasses for models
from dataclasses import dataclass, field
from typing import List, Optional

@dataclass
class RetrievalRequest:
    knowledge_id: str
    query: str
    retrieval_setting: RetrievalSetting
    metadata_condition: Optional[MetadataFilter] = None

# Use type hints everywhere
from typing import Protocol, TypeVar, Generic

T = TypeVar('T')

class EmbeddingProvider(Protocol):
    async def encode(self, texts: List[str]) -> List[List[float]]: ...

# Use pathlib for file operations
from pathlib import Path

def load_config(config_path: Path) -> dict:
    if not config_path.exists():
        raise FileNotFoundError(f"Config not found: {config_path}")
    return yaml.safe_load(config_path.read_text())
```

#### Implement API Versioning
```python
# api/v1/__init__.py
from fastapi import APIRouter

api_v1_router = APIRouter(prefix="/api/v1")

# Include versioned routes
api_v1_router.include_router(knowledge_router, prefix="/knowledge")
api_v1_router.include_router(document_router, prefix="/document")

# api/v2/__init__.py (for future versions)
api_v2_router = APIRouter(prefix="/api/v2")
```

#### Add Monitoring and Observability
```python
# services/monitoring.py
from prometheus_client import Counter, Histogram, Gauge
import time

# Metrics
search_requests_total = Counter(
    'search_requests_total',
    'Total number of search requests',
    ['collection', 'status']
)

search_duration = Histogram(
    'search_duration_seconds',
    'Search request duration',
    ['collection']
)

active_connections = Gauge(
    'active_connections',
    'Number of active database connections'
)

# Usage
@search_duration.time()
async def search(request: RetrievalRequest):
    start_time = time.time()
    try:
        result = await knowledge_service.search(request)
        search_requests_total.labels(
            collection=request.knowledge_id,
            status='success'
        ).inc()
        return result
    except Exception as e:
        search_requests_total.labels(
            collection=request.knowledge_id,
            status='error'
        ).inc()
        raise
```

---

## üìä Implementation Priorities

### Quick Wins (1-2 weeks)
- ‚úÖ Replace print statements with logging
- ‚úÖ Add basic input validation
- ‚úÖ Create comprehensive test suite
- ‚úÖ Add Docker support
- ‚úÖ Implement basic rate limiting
- ‚úÖ Centralize configuration with validation

### Medium-term (1-2 months)
- ‚úÖ Implement dependency injection
- ‚úÖ Add comprehensive caching layer
- ‚úÖ Improve authentication/authorization (JWT)
- ‚úÖ Set up CI/CD pipeline
- ‚úÖ Add monitoring and observability
- ‚úÖ Implement connection pooling
- ‚úÖ Refactor large services

### Long-term (3+ months)
- ‚úÖ Microservices architecture migration
- ‚úÖ GraphQL API implementation
- ‚úÖ Advanced security features (OAuth, SAML)
- ‚úÖ Performance optimization at scale
- ‚úÖ Multi-tenant support
- ‚úÖ Advanced analytics and reporting

---

## üìö Additional Resources

### Best Practices
- [FastAPI Best Practices](https://fastapi.tiangolo.com/tutorial/)
- [Python Type Hints](https://docs.python.org/3/library/typing.html)
- [Test-Driven Development](https://testdriven.io/)

### Security
- [OWASP Top 10](https://owasp.org/www-project-top-ten/)
- [FastAPI Security](https://fastapi.tiangolo.com/tutorial/security/)

### Performance
- [FastAPI Performance](https://fastapi.tiangolo.com/benchmarks/)
- [Python Performance Tips](https://wiki.python.org/moin/PythonSpeed/PerformanceTips)

### Deployment
- [Docker Best Practices](https://docs.docker.com/develop/dev-best-practices/)
- [Kubernetes Patterns](https://kubernetes.io/docs/concepts/)

---

## ü§ù Contributing

When implementing these improvements:

1. **Start with Quick Wins**: Focus on high-impact, low-effort changes first
2. **Test Thoroughly**: Add tests for each improvement
3. **Document Changes**: Update AGENTS.md and this document as you go
4. **Incremental Progress**: Break large changes into smaller, reviewable PRs
5. **Monitor Impact**: Track performance and metrics after changes

---

*Last updated: January 12, 2026*
